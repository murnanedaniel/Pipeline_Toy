diff --git a/.ipynb_checkpoints/README-checkpoint.md b/.ipynb_checkpoints/README-checkpoint.md
index 947fe1a..24ab510 100644
--- a/.ipynb_checkpoints/README-checkpoint.md
+++ b/.ipynb_checkpoints/README-checkpoint.md
@@ -167,8 +167,7 @@ dependencies correctly handled. Go ahead and commit, push and tag this experimen
 git ac -m "Size 36 convNet"
 dvc push
 git tag -a "36-convnet" -m "Size 36 convNet"
-git push --tags
-git push
+git push --follow-tags
 ```
 Of course, all of these commands could be combined in a simple alias, but I use them here as a reminder of what is happening. If you would like a fun shortcut, add the following to your .git_config in the HOME directory. 
 
@@ -178,6 +177,7 @@ Of course, all of these commands could be combined in a simple alias, but I use
                    git add -A && git commit -m $1 && dvc push && git tag -a $2 -m $1 && git push --follow-tags; \
                    }; f" 
 ```
+and run with `git dv "Commit_message" "tag-name"`.
 
 
 * **git commit, dvc push**
@@ -225,11 +225,27 @@ I found a good improvement from increasing the number of training epochs to > 5.
 
 
 
-### Visualising Performance 
+## Visualising Performance 
 
 * Include W&B in training
+
+To begin, ensure that W&B is installed with `pip install --user wandb`. Now we need to make the following additions to our `train.py` script:
+
+1. `import wandb` in the prelogue
+2. `wandb.init(project="convnet-toy")` before the training loop
+3. `wandb.watch(model, log="all")` in the train() function
+
+
+* Run an evaluation
 * Show some plots
 
+You can see my output with https://app.wandb.ai/murnanedaniel/convnet-toy
+
+## Hyperparameter Optimisation
+
+* Incorporate sweep agent in training.py
+* Run the sweep on interactive node
+* Submit it as batch job... debug this
 
 ### Update the Dataset
 
diff --git a/README.md b/README.md
index 947fe1a..f0b8449 100644
--- a/README.md
+++ b/README.md
@@ -167,8 +167,7 @@ dependencies correctly handled. Go ahead and commit, push and tag this experimen
 git ac -m "Size 36 convNet"
 dvc push
 git tag -a "36-convnet" -m "Size 36 convNet"
-git push --tags
-git push
+git push --follow-tags
 ```
 Of course, all of these commands could be combined in a simple alias, but I use them here as a reminder of what is happening. If you would like a fun shortcut, add the following to your .git_config in the HOME directory. 
 
@@ -178,6 +177,7 @@ Of course, all of these commands could be combined in a simple alias, but I use
                    git add -A && git commit -m $1 && dvc push && git tag -a $2 -m $1 && git push --follow-tags; \
                    }; f" 
 ```
+and run with `git dv "Commit_message" "tag-name"`.
 
 
 * **git commit, dvc push**
@@ -225,11 +225,29 @@ I found a good improvement from increasing the number of training epochs to > 5.
 
 
 
-### Visualising Performance 
+## Visualising Performance 
 
 * Include W&B in training
+
+To begin, ensure that W&B is installed with `pip install --user wandb`. 
+
+
+Now we need to make the following additions to our `train.py` script:
+1. `import wandb` in the prelogue
+2. `wandb.init(project="convnet-toy")` before the training loop
+3. `wandb.watch(model, log="all")` in the train() function
+
+
+* Run an evaluation
 * Show some plots
 
+You can see my output with https://app.wandb.ai/murnanedaniel/convnet-toy
+
+## Hyperparameter Optimisation
+
+* Incorporate sweep agent in training.py
+* Run the sweep on interactive node
+* Submit it as batch job... debug this
 
 ### Update the Dataset
 
diff --git a/src/.ipynb_checkpoints/train-checkpoint.py b/src/.ipynb_checkpoints/train-checkpoint.py
index 5e8bf9d..0b542ff 100644
--- a/src/.ipynb_checkpoints/train-checkpoint.py
+++ b/src/.ipynb_checkpoints/train-checkpoint.py
@@ -11,6 +11,8 @@ from models.convNet import Net
 import matplotlib.pyplot as plt
 import numpy as np
 
+import wandb
+
 def parse_args():
     """Parse command line arguments."""
     parser = argparse.ArgumentParser('train.py')
@@ -20,45 +22,20 @@ def parse_args():
 
     return parser.parse_args()
 
-def main():
-    
-    # Parse the command line
-    args = parse_args()
-    
-    transform = transforms.Compose(
-    [transforms.ToTensor(),
-     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
-
-    input_dir = args.input
-    
-    trainset = torchvision.datasets.CIFAR10(root=input_dir, train=True,
-                                        download=True, transform=transform)
-    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
-                                              shuffle=True, num_workers=2)
-
-    testset = torchvision.datasets.CIFAR10(root=input_dir, train=False,
-                                           download=True, transform=transform)
-    testloader = torch.utils.data.DataLoader(testset, batch_size=4,
-                                             shuffle=False, num_workers=2)
 
-    classes = ('plane', 'car', 'bird', 'cat',
-           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
-    
-    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
-    # device = torch.device('cpu')
-    print("Using ", device)
-    # model = Edge_Class_Net( input_dim=2, hidden_dim=64, n_graph_iters=4).to(device)
-    model = Net().to(device)
-    
+def train(model, device, trainloader, testloader, output_dir ):
+        
     
     criterion = nn.CrossEntropyLoss()
     optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
     
+    wandb.watch(model)
     
     for epoch in range(5):  # loop over the dataset multiple times
 
         model.train()
         running_loss = 0.0
+        train_loss, val_loss = 0., 0.
         for i, data in enumerate(trainloader, 0):
 
             # zero the parameter gradients
@@ -75,6 +52,7 @@ def main():
 
             # print statistics
             running_loss += loss.item()
+            train_loss += loss.item()
             if i % 2000 == 1999:    # print every 2000 mini-batches
                 print('[%d, %5d] loss: %.3f' %
                       (epoch + 1, i + 1, running_loss / 2000))
@@ -86,16 +64,60 @@ def main():
             images, labels = data[0].to(device), data[1].to(device)
             outputs = model(images)
             _, predicted = torch.max(outputs.data, 1)
+            loss = criterion(outputs, labels)
             total += labels.size(0)
             correct += (predicted == labels).sum().item()
+            val_loss += loss.item()
 
+        val_acc = 100 * correct / total
+        wandb.log({"train loss": train_loss, "val loss": val_loss, "val accuracy": val_acc})
         print('Accuracy of the network on the 10000 test images: %d %%' % (
-            100 * correct / total))
+            val_acc))
         
     print('Finished Training')
     
-    output_dir = args.output
+ 
     torch.save(model.state_dict(), output_dir)
+
+    
+    
+def main():
+    
+    # Parse the command line
+    args = parse_args()
+    
+    transform = transforms.Compose(
+    [transforms.ToTensor(),
+     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
+
+    input_dir = args.input
+    output_dir = args.output
+    
+    # Set up training and evaluation datasets
+    trainset = torchvision.datasets.CIFAR10(root=input_dir, train=True,
+                                        download=True, transform=transform)
+    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
+                                              shuffle=True, num_workers=2)
+
+    testset = torchvision.datasets.CIFAR10(root=input_dir, train=False,
+                                           download=True, transform=transform)
+    testloader = torch.utils.data.DataLoader(testset, batch_size=4,
+                                             shuffle=False, num_workers=2)
+
+    classes = ('plane', 'car', 'bird', 'cat',
+           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
+    
+    
+    # Initialise the model and monitoring
+    wandb.init(project="convnet-toy")
+    
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    print("Using ", device)
+    model = Net().to(device)
+
+    train(model, device, trainloader, testloader, output_dir)
+    
+    
     
 if __name__ == '__main__':
     main()
\ No newline at end of file
diff --git a/src/train.py b/src/train.py
index 5e8bf9d..0b542ff 100644
--- a/src/train.py
+++ b/src/train.py
@@ -11,6 +11,8 @@ from models.convNet import Net
 import matplotlib.pyplot as plt
 import numpy as np
 
+import wandb
+
 def parse_args():
     """Parse command line arguments."""
     parser = argparse.ArgumentParser('train.py')
@@ -20,45 +22,20 @@ def parse_args():
 
     return parser.parse_args()
 
-def main():
-    
-    # Parse the command line
-    args = parse_args()
-    
-    transform = transforms.Compose(
-    [transforms.ToTensor(),
-     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
-
-    input_dir = args.input
-    
-    trainset = torchvision.datasets.CIFAR10(root=input_dir, train=True,
-                                        download=True, transform=transform)
-    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
-                                              shuffle=True, num_workers=2)
-
-    testset = torchvision.datasets.CIFAR10(root=input_dir, train=False,
-                                           download=True, transform=transform)
-    testloader = torch.utils.data.DataLoader(testset, batch_size=4,
-                                             shuffle=False, num_workers=2)
 
-    classes = ('plane', 'car', 'bird', 'cat',
-           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
-    
-    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
-    # device = torch.device('cpu')
-    print("Using ", device)
-    # model = Edge_Class_Net( input_dim=2, hidden_dim=64, n_graph_iters=4).to(device)
-    model = Net().to(device)
-    
+def train(model, device, trainloader, testloader, output_dir ):
+        
     
     criterion = nn.CrossEntropyLoss()
     optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
     
+    wandb.watch(model)
     
     for epoch in range(5):  # loop over the dataset multiple times
 
         model.train()
         running_loss = 0.0
+        train_loss, val_loss = 0., 0.
         for i, data in enumerate(trainloader, 0):
 
             # zero the parameter gradients
@@ -75,6 +52,7 @@ def main():
 
             # print statistics
             running_loss += loss.item()
+            train_loss += loss.item()
             if i % 2000 == 1999:    # print every 2000 mini-batches
                 print('[%d, %5d] loss: %.3f' %
                       (epoch + 1, i + 1, running_loss / 2000))
@@ -86,16 +64,60 @@ def main():
             images, labels = data[0].to(device), data[1].to(device)
             outputs = model(images)
             _, predicted = torch.max(outputs.data, 1)
+            loss = criterion(outputs, labels)
             total += labels.size(0)
             correct += (predicted == labels).sum().item()
+            val_loss += loss.item()
 
+        val_acc = 100 * correct / total
+        wandb.log({"train loss": train_loss, "val loss": val_loss, "val accuracy": val_acc})
         print('Accuracy of the network on the 10000 test images: %d %%' % (
-            100 * correct / total))
+            val_acc))
         
     print('Finished Training')
     
-    output_dir = args.output
+ 
     torch.save(model.state_dict(), output_dir)
+
+    
+    
+def main():
+    
+    # Parse the command line
+    args = parse_args()
+    
+    transform = transforms.Compose(
+    [transforms.ToTensor(),
+     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
+
+    input_dir = args.input
+    output_dir = args.output
+    
+    # Set up training and evaluation datasets
+    trainset = torchvision.datasets.CIFAR10(root=input_dir, train=True,
+                                        download=True, transform=transform)
+    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
+                                              shuffle=True, num_workers=2)
+
+    testset = torchvision.datasets.CIFAR10(root=input_dir, train=False,
+                                           download=True, transform=transform)
+    testloader = torch.utils.data.DataLoader(testset, batch_size=4,
+                                             shuffle=False, num_workers=2)
+
+    classes = ('plane', 'car', 'bird', 'cat',
+           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
+    
+    
+    # Initialise the model and monitoring
+    wandb.init(project="convnet-toy")
+    
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    print("Using ", device)
+    model = Net().to(device)
+
+    train(model, device, trainloader, testloader, output_dir)
+    
+    
     
 if __name__ == '__main__':
     main()
\ No newline at end of file
